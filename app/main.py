import streamlit as st
import json
import pandas as pd
from sentence_transformers import SentenceTransformer
import faiss
from openai import OpenAI
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.prompts import PromptTemplate
from langchain.memory import ConversationBufferMemory
from langchain.chains import LLMChain
from langchain_community.vectorstores import Chroma
from streamlit_folium import st_folium
import folium
import base64

# ---- ì‚¬ì´ë“œë°” ----
from components.sidebar import render_sidebar
render_sidebar(last_map=None)

# ---------------- ë¡œê³  ìƒíƒœ ê´€ë¦¬ ----------------
if "logo_small" not in st.session_state:
    st.session_state.logo_small = False

# ---------------- ë°ì´í„° ë¡œë“œ ë° ëª¨ë¸ ì„¤ì • ----------------
@st.cache_resource
def load_embedding_model():
    return SentenceTransformer('jhgan/ko-sroberta-multitask')

@st.cache_data
def load_maemul_data():
    with open('../data/processed/maemul_clear.json', 'r', encoding='utf-8') as f:
        return pd.DataFrame(json.load(f))

@st.cache_resource
def build_faiss_index(data, _model):
    embeddings = _model.encode((data['ì´ë¦„'] + ' ' + data['ìœ í˜•']).tolist()).astype('float32')
    index = faiss.IndexFlatL2(embeddings.shape[1])
    index.add(embeddings)
    return index

embedding_model = load_embedding_model()
maemul_df = load_maemul_data()
faiss_index = build_faiss_index(maemul_df, embedding_model)

embedding_function = OpenAIEmbeddings(api_key=st.secrets["OPENAI_API_KEY"])
db = Chroma(persist_directory="./chroma_db", embedding_function=embedding_function)

client = OpenAI(api_key=st.secrets["OPENAI_API_KEY"])

# ---------------- Moderation ----------------
def moderate_message(message):
    response = client.moderations.create(
        model="text-moderation-latest",
        input=message
    )
    result = response.results[0]
    if result.flagged:
        categories = [cat for cat in result.categories.__dict__ if getattr(result.categories, cat)]
        return False, categories
    return True, None


class ModeratedLLMChain(LLMChain):
    def moderated_and_generate(self, user_input):
        is_safe, categories = moderate_message(user_input)
        if not is_safe:
            st.warning(f"ë¶€ì ì ˆí•œ ë©”ì‹œì§€ë¡œ ì°¨ë‹¨ë¨: {categories}")
            return ""
        response = self.predict(question=user_input)
        is_safe_ai, categories_ai = moderate_message(response)
        if not is_safe_ai:
            st.warning(f"AIì˜ ë¶€ì ì ˆ ì‘ë‹µ ì°¨ë‹¨ë¨: {categories_ai}")
            return ""
        return response

# ---------------- ë¶€ë™ì‚° ì§€ì‹ ë‹µë³€ ----------------
def answer_real_estate_question(question):
    relevant_docs = db.max_marginal_relevance_search(question, k=3)
    context = "\n\n".join([
        f"[ì¶œì²˜: {doc.metadata.get('source', 'ì•Œ ìˆ˜ ì—†ìŒ')} í˜ì´ì§€: {doc.metadata.get('page', 'ì•Œ ìˆ˜ ì—†ìŒ')}]\n{doc.page_content}"
        for doc in relevant_docs
    ])

    prompt = f"""
    ë„ˆëŠ” ëŒ€í•œë¯¼êµ­ ë¶€ë™ì‚° ì „ë¬¸ê°€ë¡œì„œ, ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì¹œì ˆí•˜ê³  ëª…í™•í•˜ê²Œ ë‹µë³€í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì•¼.
    ì •ë³´ê°€ ì•„ì˜ˆ ì—†ìœ¼ë©´ 'ë¶€ë™ì‚° ê´€ë ¨ ì§ˆë¬¸ì„ í•´ì£¼ì„¸ìš”.' ë¼ê³ ë§Œ ë‹µë³€í•´.
    
    ì •ë³´:
    {context}
    
    ì§ˆë¬¸:
    {question}
    
    ë‹µë³€:
    """

    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.2
    )
    return response.choices[0].message.content.strip()

# ---------------- ë§¤ë¬¼ ì¶”ì²œ ë° ì§€ë„ ì‹œê°í™” ----------------
def search_maemul(query, top_k=3):
    query_vec = embedding_model.encode([query]).astype('float32')
    _, indices = faiss_index.search(query_vec, top_k)
    return maemul_df.iloc[indices[0]]

def safe_value(value):
    if pd.isna(value) or value is None:
        return "ì •ë³´ ì—†ìŒ"
    return f"{int(value):,}" if isinstance(value, (int, float)) else str(value)

def display_map_and_list(coords_df):
    if coords_df.empty:
        st.warning("ì§€ë„ì— í‘œì‹œí•  ë§¤ë¬¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return None

    locations = list(zip(coords_df["ìœ„ë„"], coords_df["ê²½ë„"]))
    col1, col2 = st.columns([2, 1])

    with col1:
        m = folium.Map(location=[coords_df["ìœ„ë„"].mean(), coords_df["ê²½ë„"].mean()], zoom_start=15)

        for _, row in coords_df.iterrows():
            min_price = safe_value(row.get('ìµœì†Œ ë¶„ì–‘ ê°€ê²©') or row.get('ìµœì†Œ ë§¤ë§¤ ê°€ê²©'))
            max_price = safe_value(row.get('ìµœëŒ€ ë¶„ì–‘ ê°€ê²©') or row.get('ìµœëŒ€ ë§¤ë§¤ ê°€ê²©'))
            bunyang_date = safe_value(row.get('ë¶„ì–‘ ì—°ì›”') or row.get('ì¤€ê³µ ì—°ì›”'))

            tooltip_text = (
                f"<b>ë§¤ë¬¼ëª…:</b> {row.get('ì´ë¦„')}<br>"
                f"<b>ìœ í˜•:</b> {row.get('ìœ í˜•')}<br>"
                f"<b>ë¶„ì–‘ê°€ê²©:</b> {min_price} ~ {max_price}ë§Œì›<br>"
                f"<b>ë¶„ì–‘ì—°ì›”:</b> {bunyang_date}<br>"
            )

            folium.Marker(
                [row["ìœ„ë„"], row["ê²½ë„"]],
                tooltip=tooltip_text,
                icon=folium.DivIcon(html=f"""<div style="font-size: 40px;">ğŸ </div>""")
            ).add_to(m)

        m.fit_bounds(locations)
        st_folium(m, width=700, height=500)

    with col2:
        st.markdown("### ğŸ“‹ ë§¤ë¬¼ ë¦¬ìŠ¤íŠ¸")
        for idx, row in coords_df.iterrows():
            min_price = safe_value(row.get('ìµœì†Œ ë¶„ì–‘ ê°€ê²©') or row.get('ìµœì†Œ ë§¤ë§¤ ê°€ê²©'))
            max_price = safe_value(row.get('ìµœëŒ€ ë¶„ì–‘ ê°€ê²©') or row.get('ìµœëŒ€ ë§¤ë§¤ ê°€ê²©'))
            bunyang_date = safe_value(row.get('ë¶„ì–‘ ì—°ì›”') or row.get('ì¤€ê³µ ì—°ì›”'))

            if st.button(f"ğŸ¢ {row.get('ì´ë¦„')}", key=f"item_{idx}"):
                st.session_state["selected_idx"] = idx

            st.markdown(
                f"""
                <div style="padding:10px; margin-bottom:10px; border:1px solid #ccc; border-radius:5px; background-color:#f9f9f9;">
                    ğŸ“ ìœ í˜•: {row.get('ìœ í˜•')}<br>
                    ğŸ’° ê°€ê²©: {min_price} ~ {max_price}ë§Œì›<br>
                    ğŸ—“ï¸ ë¶„ì–‘ì—°ì›”: {bunyang_date}<br>
                </div>
                """, unsafe_allow_html=True
            )

# ---------------- ì§ˆë¬¸ ìœ í˜• íŒë‹¨ ----------------
def classify_question(question):
    knowledge_keywords = ["ì „ì„¸ì‚¬ê¸°", "ì²­ì•½", "ì„¸ê¸ˆ", "ë²•ë¥ ", "ê³„ì•½", "ë“±ê¸°", "í•´ì§€"]
    property_keywords = ["ì¶”ì²œ", "ë§¤ë¬¼", "ì§‘", "ì•„íŒŒíŠ¸", "ë¹Œë¼", "ë¶€ë™ì‚°", "ìœ„ì¹˜"]

    if any(word in question for word in knowledge_keywords):
        return "knowledge"
    elif any(word in question for word in property_keywords):
        return "property"
    return "general"

# ---------------- Streamlit ì¸í„°í˜ì´ìŠ¤ ----------------
chat_model = ChatOpenAI(
    model_name="gpt-3.5-turbo",
    api_key=st.secrets["OPENAI_API_KEY"],
    temperature=0.5
)

my_prompt = PromptTemplate(
    input_variables=["chat_history", "question"],
    template="""
    ë„ˆëŠ” ì„œìš¸ì‹œ ì†¡íŒŒêµ¬ ë¶€ë™ì‚°ì— íŠ¹í™”ëœ ì¹œì ˆí•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì•¼.
    ì´ì „ ëŒ€í™”: {chat_history}
    ì‚¬ëŒ: {question}
    AI ì–´ì‹œìŠ¤í„´íŠ¸:
    """
)

if "pre_memory" not in st.session_state:
    st.session_state.pre_memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)

moderated_chain = ModeratedLLMChain(llm=chat_model, prompt=my_prompt, memory=st.session_state.pre_memory)

import streamlit as st
import base64

# ---------------- ë¡œê³  ìƒíƒœ ê´€ë¦¬ ----------------
if "logo_small" not in st.session_state:
    st.session_state.logo_small = False

# Base64 ì´ë¯¸ì§€ ì²˜ë¦¬
def get_base64_image(image_path):
    with open(image_path, "rb") as img_file:
        return base64.b64encode(img_file.read()).decode()

image_base64 = get_base64_image("../data/image/logo.png")

# ---------------- CSS ì •ì˜ ----------------
st.markdown(f"""
    <style>
        /* ê³µí†µ ë¡œê³  ì»¨í…Œì´ë„ˆ */
        .logo-container {{
            display: flex;
            justify-content: center;
            align-items: center;
            width: 100%;
            transition: all 0.5s ease-in-out;
        }}

        /* í° ë¡œê³  (ì¤‘ì•™ ë°°ì¹˜) */
        .logo-big {{
            margin-top: 150px;
            position: relative;
        }}

        /* ì‘ì€ ë¡œê³  (í™”ë©´ ê³ ì •) */
        .logo-small {{
            position: fixed;
            top: 20px;
            left: 20px;
            background-color: #ffffff;  /* ë°°ê²½ìƒ‰ ì§€ì • */
            padding: 10px;
            margin-top:10px;
            border-radius: 8px;
            z-index: 9999;
            transition: all 0.5s ease-in-out;
        }}

        /* ë³¸ë¬¸ê³¼ì˜ ê°„ê²© í™•ë³´ */
        .main-container {{
            transition: margin-top 0.5s ease-in-out;
            margin-top: { '150px' if not st.session_state.logo_small else '80px' };
        }}
    </style>
""", unsafe_allow_html=True)

# ---------------- ë¡œê³  ë Œë”ë§ ----------------
if st.session_state.logo_small:
    logo_class = "logo-container logo-small"
    logo_width = 150
else:
    logo_class = "logo-container logo-big"
    logo_width = 600

st.markdown(f"""
    <div class="{logo_class}">
        <img src="data:image/png;base64,{image_base64}" width="{logo_width}">
    </div>
""", unsafe_allow_html=True)

# ---------------- í˜ì´ì§€ ì»¨í…ì¸  ì˜ì—­ (ì˜ˆì‹œ) ----------------
st.markdown('<div class="main-container">', unsafe_allow_html=True)

st.markdown('</div>', unsafe_allow_html=True)




# âœ… CSS ì»¤ìŠ¤í„°ë§ˆì´ì§•
st.markdown(
    """
    <style>
        .ek8upll0 {
            display: flex;
            justify-content: center;
            align-items: center;
            }
        .st-emotion-cache-1d2o6qs { max-width: 1400px !important; }
        .st-emotion-cache-1104ytp p { font-size : 20px; }
    </style>
    """,
    unsafe_allow_html=True
)

# âœ… ë©”ì‹œì§€ ë Œë”ë§ (ì§€ë„ í¬í•¨)
if "messages" not in st.session_state:
    st.session_state.messages = []

for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.write(msg["content"])
        if msg.get("map_data") is not None:
            display_map_and_list(msg["map_data"])

# âœ… ì‚¬ìš©ì ì§ˆë¬¸ ì…ë ¥
user_prompt = st.chat_input("ë¶€ë™ì‚° ê´€ë ¨ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”.")

if user_prompt:
    st.session_state.logo_small = True  # ë¡œê³  ì‘ê²Œ ì „í™˜

    st.session_state.messages.append({
        "role": "user",
        "content": user_prompt
    })

    with st.chat_message("user"):
        st.write(user_prompt)

    with st.chat_message("assistant"):
        with st.spinner("ë‹µë³€ì„ ì¤€ë¹„ ì¤‘ì…ë‹ˆë‹¤..."):
            try:
                question_type = classify_question(user_prompt)

                if question_type == "property":
                    ai_response = moderated_chain.moderated_and_generate(user_prompt)
                    recommended_maemul = search_maemul(user_prompt)

                    st.write(ai_response)

                    display_map_and_list(recommended_maemul)

                    st.session_state.messages.append({
                        "role": "assistant",
                        "content": ai_response,
                        "map_data": recommended_maemul
                    })

                elif question_type == "knowledge":
                    answer = answer_real_estate_question(user_prompt)

                    st.write(answer)

                    st.session_state.messages.append({
                        "role": "assistant",
                        "content": answer
                    })

                else:
                    ai_response = moderated_chain.moderated_and_generate(user_prompt)

                    st.write(ai_response)

                    st.session_state.messages.append({
                        "role": "assistant",
                        "content": ai_response
                    })

            except Exception as e:
                st.error(f"ì—ëŸ¬ ë°œìƒ: {e}")